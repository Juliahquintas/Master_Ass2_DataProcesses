{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a14c749",
   "metadata": {},
   "source": [
    "## Pipeline SVM: Preprocesamiento, BÃºsqueda de HiperparÃ¡metros y EvaluaciÃ³n\n",
    "\n",
    "En las siguientes celdas se carga el dataset limpio, se define un pipeline que incluye preprocesamiento (escalado) y un clasificador SVM, se realiza bÃºsqueda sistemÃ¡tica de hiperparÃ¡metros con `GridSearchCV`, y se evalÃºa el mejor modelo en un set de prueba. Se guardan resultados resumidos para extraer conclusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM pipeline: carga datos, preprocesado, GridSearch y evaluaciÃ³n\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/diabetes_012_cleaned.csv') \n",
    "\n",
    "with open('../data/dictionary.json', 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# Rutas\n",
    "DATA_PATH = Path('..') / 'data' / 'diabetes_012_processed.csv'\n",
    "DICT_PATH = Path('..') / 'data' / 'dictionary.json'\n",
    "OUT_DIR = Path('.') / 'svm_results'\n",
    "OUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabc2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset_undersampling_smote(df, target_col='Diabetes', random_state=42):\n",
    "    \"\"\"\n",
    "    Balancear dataset combinando undersampling y SMOTE:\n",
    "    - Clase 0 (No diabÃ©tico): undersampling - reducir hasta ~1.5x de la clase mayoritaria entre 1 y 2\n",
    "    - Clase 1 (Pre-diabetes): SMOTE - generar sintÃ©ticas hasta alcanzar ~90% de la clase 2 (Diabetes)\n",
    "    - Clase 2 (Diabetes): mantener sin cambios (referencia)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame con la columna target\n",
    "    target_col : str\n",
    "        Nombre de la columna objetivo (default: 'Diabetes')\n",
    "    random_state : int\n",
    "        Para reproducibilidad\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame balanceado\n",
    "    \"\"\"\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"BALANCEO DE DATASET: UNDERSAMPLING + SMOTE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Separar features y target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Contar clases\n",
    "    print(f\"\\nðŸ“Š DistribuciÃ³n inicial:\")\n",
    "    class_counts = y.value_counts().sort_index()\n",
    "    for cls, count in class_counts.items():\n",
    "        pct = 100 * count / len(y)\n",
    "        print(f\"  Clase {cls}: {count:,} muestras ({pct:.2f}%)\")\n",
    "    \n",
    "    class_2_count = class_counts.get(2, 0)\n",
    "    target_count = int(class_2_count * 0.9)  # 90% de la clase 2\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Objetivo final:\")\n",
    "    print(f\"  - Clase 0: {target_count:,} muestras (undersampling)\")\n",
    "    print(f\"  - Clase 1: {target_count:,} muestras (SMOTE)\")\n",
    "    print(f\"  - Clase 2: {class_2_count:,} muestras (sin cambios)\")\n",
    "    \n",
    "    # Paso 1: Undersampling para clase 0\n",
    "    print(f\"\\n[1/2] Aplicando undersampling a clase 0...\")\n",
    "    X_0 = X[y == 0]\n",
    "    y_0 = y[y == 0]\n",
    "    \n",
    "    # Muestrear aleatoriamente\n",
    "    sample_idx = np.random.RandomState(random_state).choice(\n",
    "        len(X_0), size=target_count, replace=False\n",
    "    )\n",
    "    X_0_sampled = X_0.iloc[sample_idx]\n",
    "    y_0_sampled = y_0.iloc[sample_idx]\n",
    "    print(f\"âœ“ Clase 0 reducida: {len(y_0)} â†’ {len(y_0_sampled):,} muestras\")\n",
    "    \n",
    "    # Paso 2: SMOTE para clase 1\n",
    "    print(f\"\\n[2/2] Aplicando SMOTE a clase 1...\")\n",
    "    \n",
    "    # Combinar clase 0 reducida con clase 1 original\n",
    "    X_temp = pd.concat([X_0_sampled, X[y == 1]])\n",
    "    y_temp = pd.concat([y_0_sampled, y[y == 1]])\n",
    "    \n",
    "    # Aplicar SMOTE solo a clase 1 (generar hasta target_count)\n",
    "    sampling_strategy = {\n",
    "        1: target_count  # Generar sintÃ©ticas para clase 1 hasta este nÃºmero\n",
    "    }\n",
    "    \n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state, k_neighbors=5)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X_temp, y_temp)\n",
    "    \n",
    "    count_1_before = (y_temp == 1).sum()\n",
    "    count_1_after = (y_balanced == 1).sum()\n",
    "    print(f\"âœ“ Clase 1 expandida: {count_1_before} â†’ {count_1_after:,} muestras (SMOTE sintÃ©ticas: {count_1_after - count_1_before:,})\")\n",
    "    \n",
    "    # Paso 3: AÃ±adir clase 2 sin cambios\n",
    "    print(f\"\\n[3/3] AÃ±adiendo clase 2 (sin cambios)...\")\n",
    "    X_balanced = pd.concat([X_balanced, X[y == 2]])\n",
    "    y_balanced = pd.concat([y_balanced, y[y == 2]])\n",
    "    print(f\"âœ“ Clase 2 mantenida: {class_2_count:,} muestras\")\n",
    "    \n",
    "    # Combinar en DataFrame\n",
    "    df_balanced = X_balanced.copy()\n",
    "    df_balanced[target_col] = y_balanced.values\n",
    "    df_balanced = df_balanced.reset_index(drop=True)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸ“ˆ DistribuciÃ³n final:\")\n",
    "    print(\"=\" * 70)\n",
    "    final_counts = df_balanced[target_col].value_counts().sort_index()\n",
    "    total = len(df_balanced)\n",
    "    for cls, count in final_counts.items():\n",
    "        pct = 100 * count / total\n",
    "        print(f\"  Clase {cls}: {count:,} muestras ({pct:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nâœ… Dataset balanceado: {len(df)} â†’ {len(df_balanced):,} muestras\")\n",
    "    print(f\"   Incremento: {len(df_balanced) - len(df):,} muestras nuevas (sintÃ©ticas + reducidas)\")\n",
    "    \n",
    "    return df_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be8424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BALANCEANDO DATASET...\n",
      "======================================================================\n",
      "======================================================================\n",
      "BALANCEO DE DATASET: UNDERSAMPLING + SMOTE\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š DistribuciÃ³n inicial:\n",
      "  Clase 0: 213,703 muestras (84.24%)\n",
      "  Clase 1: 4,631 muestras (1.83%)\n",
      "  Clase 2: 35,346 muestras (13.93%)\n",
      "\n",
      "ðŸŽ¯ Objetivo final:\n",
      "  - Clase 0: 31,811 muestras (undersampling)\n",
      "  - Clase 1: 31,811 muestras (SMOTE)\n",
      "  - Clase 2: 35,346 muestras (sin cambios)\n",
      "\n",
      "[1/2] Aplicando undersampling a clase 0...\n",
      "âœ“ Clase 0 reducida: 213703 â†’ 31,811 muestras\n",
      "\n",
      "[2/2] Aplicando SMOTE a clase 1...\n",
      "âœ“ Clase 1 expandida: 4631 â†’ 31,811 muestras (SMOTE sintÃ©ticas: 27,180)\n",
      "\n",
      "[3/3] AÃ±adiendo clase 2 (sin cambios)...\n",
      "âœ“ Clase 2 mantenida: 35,346 muestras\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ DistribuciÃ³n final:\n",
      "======================================================================\n",
      "  Clase 0: 31,811 muestras (32.14%)\n",
      "  Clase 1: 31,811 muestras (32.14%)\n",
      "  Clase 2: 35,346 muestras (35.71%)\n",
      "\n",
      "âœ… Dataset balanceado: 253680 â†’ 98,968 muestras\n",
      "   Incremento: -154,712 muestras nuevas (sintÃ©ticas + reducidas)\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n\" + \"=\" * 70)\n",
    "# print(\"BALANCEANDO DATASET...\")\n",
    "# print(\"=\" * 70)\n",
    "# df_balanced = balance_dataset_undersampling_smote(df, target_col='Diabetes', random_state=42)\n",
    "\n",
    "# df_balanced.to_csv('../data/diabetes_012_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53e5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/5] Cargando datos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Datos cargados: 98968 muestras, 22 columnas\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "with open(DICT_PATH, 'r', encoding='utf-8') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Datos cargados: {df.shape[0]} muestras, {df.shape[1]} columnas\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24568ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Diabetes'])\n",
    "y = df['Diabetes']\n",
    "\n",
    "ordinal_vars = [k for k, v in data_dict.items() if v.get('type') == 'ordinal' and k in X.columns]\n",
    "categorical_vars = [k for k, v in data_dict.items() if v.get('type') == 'categorical' and k in X.columns]\n",
    "numerical_vars = [k for k, v in data_dict.items() if v.get('type') == 'numerical' and k in X.columns]\n",
    "\n",
    "num_features = numerical_vars + ordinal_vars\n",
    "cat_features = categorical_vars\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_features),\n",
    "], remainder='drop')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e6b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', SVC(probability=True))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    # 'clf__C': [0.1, 1, 10, 100],\n",
    "    # 'clf__kernel': ['linear', 'rbf'],\n",
    "    # 'clf__gamma': ['scale', 'auto']\n",
    "    'clf__C': [1, 10],\n",
    "    'clf__kernel': ['linear'],\n",
    "    'clf__gamma': ['scale']\n",
    "}\n",
    "\n",
    "n_combinations = len(param_grid['clf__C']) * len(param_grid['clf__kernel']) * len(param_grid['clf__gamma'])\n",
    "cv_folds = 5\n",
    "total_fits = n_combinations * cv_folds\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv_folds,\n",
    "    scoring='f1_macro'\n",
    "    # n_jobs=1\n",
    "    # verbose=0\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23323ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â³ Entrenando:   0%|                                    | 0/10 [00:00<?, ?fit/s]"
     ]
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Resultados\n",
    "best = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af72717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ“ Mejores hiperparÃ¡metros encontrados:\")\n",
    "for param, value in grid.best_params_.items():\n",
    "    print(f\"  - {param}: {value}\")\n",
    "print(f\"\\nâœ“ Mejor score CV (f1_macro): {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar CV results\n",
    "print(\"\\n[5/5] Guardando resultados...\")\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "cv_results.to_csv(OUT_DIR / 'gridcv_results.csv', index=False)\n",
    "\n",
    "# EvaluaciÃ³n en test set\n",
    "y_pred = grid.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2477b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Test Set (SVM)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4805a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MÃ‰TRICAS EN CONJUNTO DE PRUEBA (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nâœ“ Accuracy: {acc:.4f}\")\n",
    "print(f\"âœ“ F1-Score (macro): {f1:.4f}\")\n",
    "print(f\"\\nReporte detallado por clase:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Guardar resumen de mÃ©tricas\n",
    "summary = {\n",
    "    'best_params': grid.best_params_,\n",
    "    'cv_best_score_f1_macro': float(grid.best_score_),\n",
    "    'test_accuracy': float(acc),\n",
    "    'test_f1_macro': float(f1)\n",
    "    }\n",
    "\n",
    "with open(OUT_DIR / 'summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finan-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
